# RQ1 — Effectiveness (expert alignment)

This page documents:
- which experiments were used (Exp1–Exp34)
- how precision/recall were computed from tool-vs-expert DT agreement
- where to find the per-experiment artifacts

## Artifacts
- Per-experiment DTs: `results/trees/exp*/`
- Summary metrics: `results/summary/precision_recall.csv`

## Metric computation (as in paper)
- Candidate mutated requirements are generated by discretizing numeric ranges into a uniform grid (101 values per numeric token).
- Discrete tokens (e.g., logical operators) are fully enumerated.
- Undecided is treated as inconclusive and excluded from TP/TN/FP/FN counts.

---

## Artifacts and provenance (RQ1)

**Scope.** RQ1 is computed over the **main evaluation runs** identified in the paper as **Exp1–Exp34**.

**Primary summary table**
- Precision/recall per experiment: `replication/results/summary/precision_recall.csv`

**Experiment registry (maps Exp IDs to artifact paths)**
- `replication/docs/experiments/effectiveness_runs.csv`

**Per-experiment diagnostic outputs**
- Extracted trees and Weka outputs: `replication/results/trees/`
- Representative ARFF datasets (per100): `replication/results/arff/`
- Full raw run outputs (including GA logs, multiple ARFFs, models, etc.): `replication/results/raw/exp1` … `replication/results/raw/exp34`
